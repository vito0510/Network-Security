{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d997cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix,precision_score, recall_score, f1_score,accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Dense, Input, Dropout,LeakyReLU,BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc43feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine two dataset\n",
    "df_feature = pd.read_csv('NUSW-NB15_features.csv', encoding = 'cp1252')\n",
    "df = pd.read_csv('UNSW-NB15_1.csv', names = df_feature['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdef5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert '-' to 'None'\n",
    "df.loc[df.service=='-', 'service'] = 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2827daa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the percentage of missing data\n",
    "df.isnull().sum().sort_values(ascending=False)/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e0662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the first component of the 'srcip' column by splitting the IP address string at each period.\n",
    "df['srcip'] = df['srcip'].apply(lambda x: x.split('.')[0])\n",
    "\n",
    "# Extracting the first component of the 'dstip' column by splitting the IP address string at each period.\n",
    "df['dstip'] = df['dstip'].apply(lambda x: x.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea188dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding categorical features\n",
    "df = pd.get_dummies(df, columns = ['srcip','dstip','proto','state','service'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82307df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion_matrix(y_test,y_pred):\n",
    "    \n",
    "    # Define labels for the confusion matrix\n",
    "    labels = ['Normal','Anomaly']\n",
    "    \n",
    "    # Calculate the confusion matrix\n",
    "    matrix = confusion_matrix(y_test,y_pred)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    colors = ['blue','black']\n",
    "    \n",
    "    # Create the heatmap with the confusion matrix\n",
    "    sns.heatmap(matrix,xticklabels=labels,yticklabels=labels, cmap=colors, annot=True, fmt='d')\n",
    "    \n",
    "    # Set the title and axis labels\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17c5885",
   "metadata": {},
   "source": [
    "# Logistics Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c754710",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = ['Label','Stime', 'Ltime','sport','dsport','attack_cat'],axis = 1)\n",
    "y = df['Label']\n",
    "\n",
    "# Split the data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify = y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab40bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the running time, the model used here is not a real model, it is only used to measure the running time.\n",
    "# Begin to record the time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "log_pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),  # Apply feature scaling using StandardScaler\n",
    "        (\"model\", LogisticRegression(random_state=42, max_iter = 5000))])  # Create a logistic regression model \n",
    "log_pipe.fit(X_train,y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the Running time\n",
    "duration = end_time - start_time\n",
    "\n",
    "# Print the Running time\n",
    "print(\"Running Time：\", duration, \"s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56a190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model by using grid search\n",
    "\n",
    "log_pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),          # Apply feature scaling using StandardScaler    \n",
    "        (\"model\", LogisticRegression(random_state=42, max_iter = 5000))]) # Create a logistic regression model \n",
    "\n",
    "param_grid = {'model__C': [0.01,0.1,1,10]}\n",
    "\n",
    "# Create grid search objects using 5 fold cross validation\n",
    "grid_search = GridSearchCV(log_pipe, \n",
    "                           param_grid,\n",
    "                           scoring = [\"accuracy\", \"f1\",\"recall\",\"precision\"], \n",
    "                           cv=3, \n",
    "                           refit = \"accuracy\", \n",
    "                           return_train_score = True,\n",
    "                           error_score='raise')\n",
    "\n",
    "# Fitting training data using grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output the best parameters and the corresponding scores\n",
    "print(\"Best Parameter: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef21c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rs_df = pd.DataFrame(grid_search.cv_results_)\n",
    "#The summary of the models \n",
    "log_rs_df.sort_values(\"mean_test_accuracy\", ascending=False)[[\n",
    "    'param_model__C',\n",
    "\"mean_train_accuracy\",\n",
    "\"std_train_accuracy\",\n",
    "\"mean_test_accuracy\", \n",
    "\"std_test_accuracy\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a896b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the target variable using the trained model\n",
    "y_pred = grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0aefa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the confusion matrix\n",
    "draw_confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05290770",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred,digits = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e6d3e8",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f3b5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the running time, the model used here is not a real model, it is only used to measure the running time.\n",
    "\n",
    "# Record the beginning time\n",
    "start_time = time.time()\n",
    "\n",
    "# Exacture the normal data\n",
    "normal_data = df[df['Label'] == 0]\n",
    "\n",
    "# Create a StandardScaler object for feature scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "X_nor = normal_data.drop(columns = ['Label','Stime', 'Ltime','sport','dsport','attack_cat'],axis = 1)\n",
    "y_nor = normal_data['Label']\n",
    "\n",
    "# Split normal data into training, validation, and test sets\n",
    "X_train, X_valid_test, y_train, y_valid_test = train_test_split(X_nor, y_nor, test_size=0.1,random_state=42)\n",
    "X_valid_nor, X_test_nor, y_valid_nor, y_test_nor = train_test_split(X_valid_test, y_valid_test, test_size=0.5,random_state=42)\n",
    "\n",
    "# Scale the features of the training set\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Dimensionality of the input data\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Dimensionality of the encoded representation\n",
    "encoding_dim = 7\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = keras.Input(shape=(input_dim,))\n",
    "\n",
    "# Define the encoding layer\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Define the decoding layer\n",
    "decoded = Dense(input_dim, activation='softmax')(encoded)\n",
    "\n",
    "# Create the autoencoder model\n",
    "autoencoder = keras.Model(input_layer, decoded)\n",
    "autoencoder.summary()\n",
    "\n",
    "# Set the batch size for training\n",
    "batch_size = 1280\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 20\n",
    "\n",
    "# Compile the autoencoder model with optimizer, loss function, and metrics\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error',metrics=['mae','accuracy'])\n",
    "\n",
    "# Train the autoencoder model\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                verbose = 1,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_valid, X_valid))\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the running time\n",
    "duration = end_time - start_time\n",
    "\n",
    "# Print the running time\n",
    "print(\"Running time：\", duration, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6e6b32",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate the training, validation, testing data\n",
    "\n",
    "# Exacture the normal data\n",
    "normal_data = df[df['Label'] == 0]\n",
    "\n",
    "# Create a StandardScaler object for feature scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "X_nor = normal_data.drop(columns = ['Label','Stime', 'Ltime','sport','dsport','attack_cat'],axis = 1)\n",
    "y_nor = normal_data['Label']\n",
    "\n",
    "# Split normal data into training, validation, and test sets\n",
    "X_train, X_valid_test, y_train, y_valid_test = train_test_split(X_nor, y_nor, test_size=0.2,random_state=42)\n",
    "X_valid_nor, X_test_nor, y_valid_nor, y_test_nor = train_test_split(X_valid_test, y_valid_test, test_size=0.5,random_state=42)\n",
    "\n",
    "# Scale the features of the training set\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Exacture the anomaly data\n",
    "abnormal_data = df[df['Label'] == 1]\n",
    "X_ab = abnormal_data.drop(columns = ['Label','Stime', 'Ltime','sport','dsport','attack_cat'],axis = 1)\n",
    "y_ab = abnormal_data['Label']\n",
    "\n",
    "# Split anomaly data into validation, and test sets\n",
    "X_valid_ab, X_test_ab, y_valid_ab, y_test_ab = train_test_split(X_ab, y_ab, test_size=0.5,random_state=42)\n",
    "\n",
    "# Combine normal and anamoly data for validation and test sets\n",
    "X_valid = pd.concat([X_valid_nor,X_valid_ab])\n",
    "X_test = pd.concat([X_test_nor,X_test_ab])\n",
    "y_valid = pd.concat([y_valid_nor,y_valid_ab])\n",
    "y_test = pd.concat([y_test_nor,y_test_ab])\n",
    "\n",
    "# Scale the features of the validation and test sets using the same scaler\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6bd36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality of the input data\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Dimensionality of the encoded representation\n",
    "encoding_dim = 7\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = keras.Input(shape=(input_dim,))\n",
    "\n",
    "# Define the encoding layer\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Define the decoding layer\n",
    "decoded = Dense(input_dim, activation='softmax')(encoded)\n",
    "\n",
    "# Create the autoencoder model\n",
    "autoencoder = keras.Model(input_layer, decoded)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a5185b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the batch size for training\n",
    "batch_size = 1280\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 20\n",
    "\n",
    "# Compile the autoencoder model with optimizer, loss function, and metrics\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error',metrics=['mae','accuracy'])\n",
    "\n",
    "# Train the autoencoder model\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                verbose = 1,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20501c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the training loss for each epoch\n",
    "epochs = range(1, 21)\n",
    "train_mae = history.history['mae']\n",
    "valid_mae = history.history['val_mae']\n",
    "\n",
    "# Plotting MAE versus epochs\n",
    "plt.plot(epochs, train_mae, 'b', label='Training MAE')\n",
    "plt.plot(epochs, valid_mae, 'r', label='Validation MAE')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6867e97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate a list of threshold values\n",
    "threshold = np.linspace(0,20,40)\n",
    "\n",
    "acc = []\n",
    "rec = []\n",
    "\n",
    "# Loop over each threshold value\n",
    "for t in threshold:\n",
    "    \n",
    "    # Generate predictions using the autoencoder\n",
    "    y_pred = autoencoder.predict(X_test)\n",
    "    \n",
    "    # Calculate the reconstruction error\n",
    "    y_dist = np.linalg.norm(X_test - y_pred, axis = -1)\n",
    "    \n",
    "    # Threshold the reconstruction error to classify anomalies\n",
    "    y_pred = np.where(y_dist>=t,1,0)\n",
    "    \n",
    "    # Compute and store the accuracy score\n",
    "    acc.append(accuracy_score(y_test,y_pred))\n",
    "    \n",
    "    # Compute and store the recall score\n",
    "    rec.append(recall_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db696a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the accuary score and recall score\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(threshold,acc,c='y',label='Acc')\n",
    "plt.plot(threshold,rec,c='b',label='Recall')\n",
    "plt.xlabel('threshold')\n",
    "plt.ylabel('classification score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87f14b7",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Find the index with the maximum value of the sum of 'rec' and 'acc'\n",
    "i = np.argmax(np.array(rec)+np.array(acc))\n",
    "\n",
    "# Get the threshold value corresponding to the index\n",
    "t = threshold[i]\n",
    "\n",
    "# Predict the output using the autoencoder model\n",
    "y_pred = autoencoder.predict(X_test)\n",
    "\n",
    "# Calculate the Euclidean distance between the input and predicted output\n",
    "y_dist = np.linalg.norm(X_test - y_pred, axis = -1)\n",
    "\n",
    "# Create a zip object containing a boolean indicator for anomaly and the corresponding distance\n",
    "z = zip(y_dist >= t, y_dist)\n",
    "y_label = []\n",
    "error = []\n",
    "\n",
    "# Iterate through the zip object\n",
    "for idx, (is_anomaly,y_dist) in enumerate(z):\n",
    "    if is_anomaly:\n",
    "        y_label.append(1)  # Append 1 to the 'y_label' list if it is an anomaly\n",
    "    else:\n",
    "        y_label.append(0)  # Append 0 to the 'y_label' list if it is not an anomaly\n",
    "    error.append(y_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ba244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(classification_report(y_test,y_label,digits = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2690dd11",
   "metadata": {},
   "source": [
    "# Shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb6d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(data):\n",
    "    \n",
    "    # Predict the output using the autoencoder model\n",
    "    y_pred = autoencoder.predict(data)\n",
    "    \n",
    "    # Calculate the Euclidean distance between the input and predicted output\n",
    "    y_dist = np.linalg.norm(data - y_pred, axis = -1)\n",
    "    \n",
    "    # Return the distance\n",
    "    return y_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61fda41",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "X = df.drop(columns = ['Label','Stime', 'Ltime','sport','dsport','attack_cat'],axis = 1)\n",
    "X = pd.DataFrame(scaler.fit_transform(X))\n",
    "\n",
    "# Select a sample of 100 data points\n",
    "X_sample = X.sample(100)\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_sample.columns = df.drop(columns=['Label', 'Stime', 'Ltime', 'sport', 'dsport', 'attack_cat'], axis=1).columns\n",
    "X_train.columns = df.drop(columns=['Label', 'Stime', 'Ltime', 'sport', 'dsport', 'attack_cat'], axis=1).columns\n",
    "X_sample.columns = X.columns\n",
    "\n",
    "# Sample data from X_train for SHAP analysis\n",
    "data = shap.sample(X_train, 500)\n",
    "\n",
    "# Create a SHAP KernelExplainer object\n",
    "explainer = shap.KernelExplainer(error,data)\n",
    "\n",
    "# Compute SHAP values for X_sample using the explainer\n",
    "shap_values = explainer.shap_values(X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5cc2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of feature names\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# Predict using the autoencoder model\n",
    "y_pred = autoencoder.predict(X_sample)\n",
    "\n",
    "# Compute the reconstruction error\n",
    "y_dist = np.linalg.norm(X_sample - y_pred, axis = -1)\n",
    "\n",
    "# Set the threshold for anomaly detection\n",
    "threshold = t\n",
    "\n",
    "# Identify the indices of detected anomalies\n",
    "anomalies_detected = np.where(y_dist > threshold)[0]\n",
    "\n",
    "# Select the index of the anomaly to visualize (e.g., the third anomaly)\n",
    "anomaly_index = anomalies_detected[2]\n",
    "\n",
    "# Generate the SHAP force plot for the selected anomaly\n",
    "shap.force_plot(explainer.expected_value, shap_values[anomaly_index], X_sample.iloc[anomaly_index],feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c118ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the SHAP force plot for the normal data\n",
    "shap.force_plot(explainer.expected_value, shap_values[6], X_sample.iloc[6],feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431e49f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the summary of SHAP values\n",
    "shap.summary_plot(shap_values,X_sample,feature_names=feature_names, max_display=10, plot_type=\"dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b03cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the summary of SHAP values\n",
    "shap.summary_plot(shap_values,X_sample,feature_names=feature_names, max_display=10, plot_type=\"bar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
